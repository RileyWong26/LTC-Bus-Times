{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Load Dataset\\nfile_path = \"filename.csv\"\\ndf = pd.read_csv(file_path)\\n\\ndf = df.sample(n=20000, random_state=42)\\n\\n# Features & Target\\nfeatures = [\"vehicle_id\", \"stop_id\", \"scheduled_time\", \"actual_time\", \"day\", \"day_of_year\", \"Weather\"]\\ntarget = \"delay\"\\n\\nX = df[features].fillna(0).values  # Fill missing values if any\\ny = df[target].values\\n\\n# Normalize Features\\nscaler = StandardScaler()\\nX = scaler.fit_transform(X)\\n\\n# Train-Test Split (80%-20%)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Convert to PyTorch Tensors\\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\\ny_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\\n\\n# Create DataLoaders\\nbatch_size = 64\\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\\n\\n# Define Bi-LSTM Model with Dropout and Batch Normalization\\nclass BiLSTM(nn.Module):\\n    def __init__(self, input_dim, hidden_dim, dropout_rate):\\n        super(BiLSTM, self).__init__()\\n        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\\n        self.bn = nn.BatchNorm1d(2 * hidden_dim)\\n        self.dropout = nn.Dropout(dropout_rate)\\n        self.fc = nn.Linear(2 * hidden_dim, 1)  # Output 1 value (delay prediction)\\n\\n    def forward(self, x):\\n        x = x.unsqueeze(1)  # Reshape for LSTM [batch, seq_len=1, features]\\n        lstm_out, _ = self.lstm(x)\\n        lstm_out = lstm_out[:, -1, :]  # Take last time step output\\n        lstm_out = self.bn(lstm_out)\\n        lstm_out = self.dropout(lstm_out)\\n        out = self.fc(lstm_out)\\n        return out\\n\\n# Define Objective Function for Bayesian Optimization\\ndef objective(trial):\\n    hidden_dim = trial.suggest_int(\"hidden_dim\", 16, 128)\\n    dropout_rate = trial.suggest_float(\"dropout\", 0.1, 0.5)\\n    learning_rate = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\\n\\n    model = BiLSTM(input_dim=X_train.shape[1], hidden_dim=hidden_dim, dropout_rate=dropout_rate)\\n    criterion = nn.MSELoss()\\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\\n\\n    # Train Model\\n    model.train()\\n    for epoch in range(10):  # Train for 10 epochs\\n        for batch_X, batch_y in train_loader:\\n            optimizer.zero_grad()\\n            outputs = model(batch_X)\\n            loss = criterion(outputs, batch_y)\\n            loss.backward()\\n            optimizer.step()\\n\\n    # Evaluate on Test Set\\n    model.eval()\\n    with torch.no_grad():\\n        test_preds = model(X_test_tensor)\\n        test_loss = criterion(test_preds, y_test_tensor).item()\\n    \\n    return test_loss  # Minimize MSE Loss\\n\\n# Run Bayesian Optimization\\nstudy = optuna.create_study(direction=\"minimize\")\\nstudy.optimize(objective, n_trials=20)  # Run for 20 trials\\n\\n# Get Best Hyperparameters\\nbest_params = study.best_params\\nprint(\"Best Hyperparameters:\", best_params)\\n\\n# Train Final Model with Best Hyperparameters\\nbest_model = BiLSTM(input_dim=X_train.shape[1], hidden_dim=best_params[\"hidden_dim\"], dropout_rate=best_params[\"dropout\"])\\noptimizer = optim.Adam(best_model.parameters(), lr=best_params[\"lr\"])\\ncriterion = nn.MSELoss()\\n\\n# Training Loop\\nnum_epochs = 50\\nfor epoch in range(num_epochs):\\n    best_model.train()\\n    epoch_loss = 0\\n    for batch_X, batch_y in train_loader:\\n        optimizer.zero_grad()\\n        outputs = best_model(batch_X)\\n        loss = criterion(outputs, batch_y)\\n        loss.backward()\\n        optimizer.step()\\n        epoch_loss += loss.item()\\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\\n\\n# Evaluate Final Model\\nbest_model.eval()\\nwith torch.no_grad():\\n    final_preds = best_model(X_test_tensor)\\n    final_loss = criterion(final_preds, y_test_tensor).item()\\n\\nprint(f\"Final Test Loss (MSE): {final_loss:.4f}\")'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "'''# Load Dataset\n",
    "file_path = \"filename.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = df.sample(n=20000, random_state=42)\n",
    "\n",
    "# Features & Target\n",
    "features = [\"vehicle_id\", \"stop_id\", \"scheduled_time\", \"actual_time\", \"day\", \"day_of_year\", \"Weather\"]\n",
    "target = \"delay\"\n",
    "\n",
    "X = df[features].fillna(0).values  # Fill missing values if any\n",
    "y = df[target].values\n",
    "\n",
    "# Normalize Features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train-Test Split (80%-20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define Bi-LSTM Model with Dropout and Batch Normalization\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout_rate):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.bn = nn.BatchNorm1d(2 * hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(2 * hidden_dim, 1)  # Output 1 value (delay prediction)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Reshape for LSTM [batch, seq_len=1, features]\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Take last time step output\n",
    "        lstm_out = self.bn(lstm_out)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        out = self.fc(lstm_out)\n",
    "        return out\n",
    "\n",
    "# Define Objective Function for Bayesian Optimization\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 16, 128)\n",
    "    dropout_rate = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "    model = BiLSTM(input_dim=X_train.shape[1], hidden_dim=hidden_dim, dropout_rate=dropout_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train Model\n",
    "    model.train()\n",
    "    for epoch in range(10):  # Train for 10 epochs\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate on Test Set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_preds = model(X_test_tensor)\n",
    "        test_loss = criterion(test_preds, y_test_tensor).item()\n",
    "    \n",
    "    return test_loss  # Minimize MSE Loss\n",
    "\n",
    "# Run Bayesian Optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)  # Run for 20 trials\n",
    "\n",
    "# Get Best Hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train Final Model with Best Hyperparameters\n",
    "best_model = BiLSTM(input_dim=X_train.shape[1], hidden_dim=best_params[\"hidden_dim\"], dropout_rate=best_params[\"dropout\"])\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params[\"lr\"])\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    best_model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Evaluate Final Model\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    final_preds = best_model(X_test_tensor)\n",
    "    final_loss = criterion(final_preds, y_test_tensor).item()\n",
    "\n",
    "print(f\"Final Test Loss (MSE): {final_loss:.4f}\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower Learning Rate Bound (1e-5 to 1e-3)\tPrevents instability in updates\n",
    "\n",
    "Hidden Dim Range (32-96)\tPrevents overfitting from large models\n",
    "\n",
    "Dropout Before BatchNorm\tMore effective regularization\n",
    "\n",
    "Smaller Dropout Range (0.2-0.4)\tKeeps useful information\n",
    "\n",
    "Increased Batch Size (128)\tStabilizes gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-31 14:28:27,768] A new study created in memory with name: no-name-01ffb94a-ae53-4efe-adc5-a958c4af7fd5\n",
      "[I 2025-01-31 14:28:42,443] Trial 0 finished with value: 132006.53125 and parameters: {'hidden_dim': 71, 'dropout': 0.3695921266185635, 'lr': 8.386963754610721e-05}. Best is trial 0 with value: 132006.53125.\n",
      "[I 2025-01-31 14:28:54,763] Trial 1 finished with value: 129694.5390625 and parameters: {'hidden_dim': 84, 'dropout': 0.3228789702969039, 'lr': 0.0001340818041813791}. Best is trial 1 with value: 129694.5390625.\n",
      "[I 2025-01-31 14:29:07,155] Trial 2 finished with value: 121205.578125 and parameters: {'hidden_dim': 83, 'dropout': 0.26485704225349616, 'lr': 0.0002358154738175493}. Best is trial 2 with value: 121205.578125.\n",
      "[I 2025-01-31 14:29:17,514] Trial 3 finished with value: 118734.6328125 and parameters: {'hidden_dim': 60, 'dropout': 0.3813602196079009, 'lr': 0.00030542572445757814}. Best is trial 3 with value: 118734.6328125.\n",
      "[I 2025-01-31 14:29:31,020] Trial 4 finished with value: 124279.34375 and parameters: {'hidden_dim': 71, 'dropout': 0.3629885386591768, 'lr': 0.0002259719548876062}. Best is trial 3 with value: 118734.6328125.\n",
      "[I 2025-01-31 14:29:40,016] Trial 5 finished with value: 132978.75 and parameters: {'hidden_dim': 34, 'dropout': 0.3118073565537715, 'lr': 1.559798403670482e-05}. Best is trial 3 with value: 118734.6328125.\n",
      "[I 2025-01-31 14:29:53,354] Trial 6 finished with value: 103468.1875 and parameters: {'hidden_dim': 71, 'dropout': 0.20344230250433148, 'lr': 0.0008526008726960454}. Best is trial 6 with value: 103468.1875.\n",
      "[I 2025-01-31 14:30:03,671] Trial 7 finished with value: 131562.203125 and parameters: {'hidden_dim': 49, 'dropout': 0.3690853064264703, 'lr': 0.00011594473343341628}. Best is trial 6 with value: 103468.1875.\n",
      "[I 2025-01-31 14:30:15,123] Trial 8 finished with value: 129896.28125 and parameters: {'hidden_dim': 66, 'dropout': 0.2210892254677325, 'lr': 0.00013799752154102077}. Best is trial 6 with value: 103468.1875.\n",
      "[I 2025-01-31 14:30:23,924] Trial 9 finished with value: 132412.921875 and parameters: {'hidden_dim': 37, 'dropout': 0.24902718335625448, 'lr': 8.037041429656195e-05}. Best is trial 6 with value: 103468.1875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'hidden_dim': 71, 'dropout': 0.20344230250433148, 'lr': 0.0008526008726960454}\n",
      "Epoch 1/50, Loss: 41736327.2500\n",
      "Epoch 2/50, Loss: 38309068.8086\n",
      "Epoch 3/50, Loss: 34710026.3477\n",
      "Epoch 4/50, Loss: 33570935.8711\n",
      "Epoch 5/50, Loss: 33289395.4375\n",
      "Epoch 6/50, Loss: 33143050.3320\n",
      "Epoch 7/50, Loss: 33013247.7188\n",
      "Epoch 8/50, Loss: 32942009.1797\n",
      "Epoch 9/50, Loss: 32857708.7656\n",
      "Epoch 10/50, Loss: 32816788.5000\n",
      "Epoch 11/50, Loss: 32754266.6133\n",
      "Epoch 12/50, Loss: 32723470.0703\n",
      "Epoch 13/50, Loss: 32666991.4824\n",
      "Epoch 14/50, Loss: 32545641.3555\n",
      "Epoch 15/50, Loss: 32502949.4961\n",
      "Epoch 16/50, Loss: 32413435.3750\n",
      "Epoch 17/50, Loss: 32420363.8789\n",
      "Epoch 18/50, Loss: 32280328.2969\n",
      "Epoch 19/50, Loss: 32245303.2969\n",
      "Epoch 20/50, Loss: 32117850.8535\n",
      "Epoch 21/50, Loss: 32110246.2891\n",
      "Epoch 22/50, Loss: 32061190.8047\n",
      "Epoch 23/50, Loss: 31868108.8418\n",
      "Epoch 24/50, Loss: 31815936.5430\n",
      "Epoch 25/50, Loss: 31713113.1035\n",
      "Epoch 26/50, Loss: 31780600.6992\n",
      "Epoch 27/50, Loss: 31530830.8574\n",
      "Epoch 28/50, Loss: 31432192.1445\n",
      "Epoch 29/50, Loss: 31368075.3574\n",
      "Epoch 30/50, Loss: 31229676.7969\n",
      "Epoch 31/50, Loss: 31133967.0879\n",
      "Epoch 32/50, Loss: 31010407.7148\n",
      "Epoch 33/50, Loss: 31102109.2031\n",
      "Epoch 34/50, Loss: 30919906.8906\n",
      "Epoch 35/50, Loss: 30803543.0488\n",
      "Epoch 36/50, Loss: 30732192.8203\n",
      "Epoch 37/50, Loss: 30738536.0703\n",
      "Epoch 38/50, Loss: 30695644.2793\n",
      "Epoch 39/50, Loss: 30603897.4141\n",
      "Epoch 40/50, Loss: 30525022.0781\n",
      "Epoch 41/50, Loss: 30438977.6191\n",
      "Epoch 42/50, Loss: 30301493.5078\n",
      "Epoch 43/50, Loss: 30409684.1621\n",
      "Epoch 44/50, Loss: 30250861.9980\n",
      "Epoch 45/50, Loss: 30220464.9062\n",
      "Epoch 46/50, Loss: 30097963.9023\n",
      "Epoch 47/50, Loss: 30084703.3262\n",
      "Epoch 48/50, Loss: 30040149.3008\n",
      "Epoch 49/50, Loss: 29932671.6855\n",
      "Epoch 50/50, Loss: 29987979.1270\n",
      "Final Test Loss (MSE): 93367.3281\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "file_path = \"filename.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = df.sample(n=50000, random_state=42)\n",
    "\n",
    "features = [\"vehicle_id\", \"stop_id\", \"scheduled_time\", \"day\", \"day_of_year\", \"Weather\"]\n",
    "target = \"delay\"\n",
    "\n",
    "X = df[features].fillna(0).values  # Fill missing values if any\n",
    "y = df[target].values\n",
    "\n",
    "#Normalization\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Here I convert the data to PyTorch Tensors. Also, I tried to move tensors to GPU for faster training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "#Wraps datasets into TensorDataset objects\n",
    "#Also batches data (from the dropout paper)\n",
    "batch_size = 128 \n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#Define the BiLSTM model\n",
    "#\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout_rate):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout before batch norm\n",
    "        self.bn = nn.BatchNorm1d(2 * hidden_dim)\n",
    "        self.fc = nn.Linear(2 * hidden_dim, 1)  # Output 1 value (delay prediction)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Reshape for LSTM [batch, seq_len=1, features]\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Take last time step output\n",
    "        lstm_out = self.dropout(lstm_out)  # Apply dropout\n",
    "        lstm_out = self.bn(lstm_out)  # Batch normalization\n",
    "        out = self.fc(lstm_out)\n",
    "        return out\n",
    "\n",
    "#Define Objective Function for Bayesian Optimization. I used optuna because chatGPT recommended it. Seems to be good. Otherwise, I can use BayesSearchCV.\n",
    "#Runs 5 training epochs per trial to quickly evaluate hyperparameters.\n",
    "#Returns MSE loss as the optimization target.\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 96)  # Reduced upper limit\n",
    "    dropout_rate = trial.suggest_float(\"dropout\", 0.2, 0.4)  # Better range\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)  # Lower max lr\n",
    "\n",
    "    model = BiLSTM(input_dim=X_train.shape[1], hidden_dim=hidden_dim, dropout_rate=dropout_rate).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train Model (Only 5 epochs for speed)\n",
    "    model.train()\n",
    "    for epoch in range(5):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate on Test Set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_preds = model(X_test_tensor)\n",
    "        test_loss = criterion(test_preds, y_test_tensor).item()\n",
    "    \n",
    "    return test_loss  # Minimize MSE Loss\n",
    "\n",
    "#I ran 10 trials to find the best hyperparameters\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)  # Reduced to 10 trials\n",
    "\n",
    "#Get Best Hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "#Train Final Model with Best Hyperparameters\n",
    "best_model = BiLSTM(input_dim=X_train.shape[1], hidden_dim=best_params[\"hidden_dim\"], dropout_rate=best_params[\"dropout\"]).to(device)\n",
    "#Initializes the Adam optimizer with the best hyperparameters found by Bayesian optimization.\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params[\"lr\"])\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 50  #Might be overfitting?\n",
    "for epoch in range(num_epochs):\n",
    "    best_model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "#Evaluate Final Model\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    final_preds = best_model(X_test_tensor)\n",
    "    final_loss = criterion(final_preds, y_test_tensor).item()\n",
    "\n",
    "print(f\"Final Test Loss (MSE): {final_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REDUCED EPOCH (50 to 15) TO SEE IF OVERFITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-31 14:32:47,657] A new study created in memory with name: no-name-6e0e31e5-963d-4415-aefa-dfce65a3a687\n",
      "[I 2025-01-31 14:32:53,927] Trial 0 finished with value: 162494.40625 and parameters: {'hidden_dim': 92, 'dropout': 0.24392271372279434, 'lr': 1.4368823640642128e-05}. Best is trial 0 with value: 162494.40625.\n",
      "[I 2025-01-31 14:32:57,558] Trial 1 finished with value: 162465.421875 and parameters: {'hidden_dim': 35, 'dropout': 0.2440503454236962, 'lr': 2.625622702034492e-05}. Best is trial 1 with value: 162465.421875.\n",
      "[I 2025-01-31 14:33:02,816] Trial 2 finished with value: 153268.515625 and parameters: {'hidden_dim': 91, 'dropout': 0.29517088354898396, 'lr': 0.0005056394121858034}. Best is trial 2 with value: 153268.515625.\n",
      "[I 2025-01-31 14:33:06,843] Trial 3 finished with value: 160515.734375 and parameters: {'hidden_dim': 46, 'dropout': 0.31909395448807154, 'lr': 0.0003356216927087496}. Best is trial 2 with value: 153268.515625.\n",
      "[I 2025-01-31 14:33:12,565] Trial 4 finished with value: 162352.84375 and parameters: {'hidden_dim': 81, 'dropout': 0.36912310712496665, 'lr': 6.550114931226505e-05}. Best is trial 2 with value: 153268.515625.\n",
      "[I 2025-01-31 14:33:18,534] Trial 5 finished with value: 162443.25 and parameters: {'hidden_dim': 86, 'dropout': 0.3234910722588886, 'lr': 3.724501825492443e-05}. Best is trial 2 with value: 153268.515625.\n",
      "[I 2025-01-31 14:33:23,275] Trial 6 finished with value: 161912.21875 and parameters: {'hidden_dim': 33, 'dropout': 0.32012682838061024, 'lr': 0.00020975811970005398}. Best is trial 2 with value: 153268.515625.\n",
      "[I 2025-01-31 14:33:28,139] Trial 7 finished with value: 161745.03125 and parameters: {'hidden_dim': 51, 'dropout': 0.312740328309714, 'lr': 0.00020391611889550537}. Best is trial 2 with value: 153268.515625.\n",
      "[I 2025-01-31 14:33:32,741] Trial 8 finished with value: 162373.875 and parameters: {'hidden_dim': 62, 'dropout': 0.37788057500899835, 'lr': 7.26057969477026e-05}. Best is trial 2 with value: 153268.515625.\n",
      "[I 2025-01-31 14:33:37,561] Trial 9 finished with value: 162495.390625 and parameters: {'hidden_dim': 46, 'dropout': 0.2908976342575896, 'lr': 2.5427666600219667e-05}. Best is trial 2 with value: 153268.515625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'hidden_dim': 91, 'dropout': 0.29517088354898396, 'lr': 0.0005056394121858034}\n",
      "Epoch 1/15, Loss: 16110318.1641\n",
      "Epoch 2/15, Loss: 16042438.3477\n",
      "Epoch 3/15, Loss: 15889039.3828\n",
      "Epoch 4/15, Loss: 15625438.8477\n",
      "Epoch 5/15, Loss: 15240629.8398\n",
      "Epoch 6/15, Loss: 14768443.8125\n",
      "Epoch 7/15, Loss: 14276772.4102\n",
      "Epoch 8/15, Loss: 13842328.8125\n",
      "Epoch 9/15, Loss: 13491036.2891\n",
      "Epoch 10/15, Loss: 13222277.5781\n",
      "Epoch 11/15, Loss: 13030676.9492\n",
      "Epoch 12/15, Loss: 12927351.0156\n",
      "Epoch 13/15, Loss: 12826640.8242\n",
      "Epoch 14/15, Loss: 12799336.8125\n",
      "Epoch 15/15, Loss: 12751756.0508\n",
      "Final Test Loss (MSE): 132760.0781\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load Dataset\n",
    "file_path = \"filename.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = df.sample(n=20000, random_state=42)\n",
    "\n",
    "# Features & Target\n",
    "features = [\"vehicle_id\", \"stop_id\", \"scheduled_time\", \"day\", \"day_of_year\", \"Weather\"]\n",
    "target = \"delay\"\n",
    "\n",
    "X = df[features].fillna(0).values  # Fill missing values if any\n",
    "y = df[target].values\n",
    "\n",
    "# Normalize Features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train-Test Split (80%-20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 128  # Increased batch size for stability\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define Optimized Bi-LSTM Model\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout_rate):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout before batch norm\n",
    "        self.bn = nn.BatchNorm1d(2 * hidden_dim)\n",
    "        self.fc = nn.Linear(2 * hidden_dim, 1)  # Output 1 value (delay prediction)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Reshape for LSTM [batch, seq_len=1, features]\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Take last time step output\n",
    "        lstm_out = self.dropout(lstm_out)  # Apply dropout\n",
    "        lstm_out = self.bn(lstm_out)  # Batch normalization\n",
    "        out = self.fc(lstm_out)\n",
    "        return out\n",
    "\n",
    "# Define Objective Function for Bayesian Optimization\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 96)  # Reduced upper limit\n",
    "    dropout_rate = trial.suggest_float(\"dropout\", 0.2, 0.4)  # Better range\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)  # Lower max lr\n",
    "\n",
    "    model = BiLSTM(input_dim=X_train.shape[1], hidden_dim=hidden_dim, dropout_rate=dropout_rate).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train Model (Only 5 epochs for speed)\n",
    "    model.train()\n",
    "    for epoch in range(5):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate on Test Set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_preds = model(X_test_tensor)\n",
    "        test_loss = criterion(test_preds, y_test_tensor).item()\n",
    "    \n",
    "    return test_loss  # Minimize MSE Loss\n",
    "\n",
    "# Run Bayesian Optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)  # Reduced to 10 trials\n",
    "\n",
    "# Get Best Hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train Final Model with Best Hyperparameters (15 epochs for better learning)\n",
    "best_model = BiLSTM(input_dim=X_train.shape[1], hidden_dim=best_params[\"hidden_dim\"], dropout_rate=best_params[\"dropout\"]).to(device)\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_params[\"lr\"])\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 15  # Reduced from 50 to avoid overfitting\n",
    "for epoch in range(num_epochs):\n",
    "    best_model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Evaluate Final Model\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    final_preds = best_model(X_test_tensor)\n",
    "    final_loss = criterion(final_preds, y_test_tensor).item()\n",
    "\n",
    "print(f\"Final Test Loss (MSE): {final_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
